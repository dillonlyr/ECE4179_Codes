{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def seed_all(seed=0):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.5])\n",
    "w = torch.tensor([0.2], requires_grad=True)\n",
    "w.grad\n",
    "\n",
    "y = torch.tensor([0.5])\n",
    "\n",
    "yhat = x * w\n",
    "loss = (yhat - y)**2\n",
    "\n",
    "# loss.grad_fn.next_functions[0][0].next_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD([w], lr=0.01)\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/fashionmnist_train.csv').to_numpy()\n",
    "idx = np.random.randint(0, data.shape[0])\n",
    "plt.imshow(data[idx, 1:].reshape(28, 28), cmap='gray')\n",
    "plt.title(classes[data[idx, 0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data = pd.read_csv(csv_file).to_numpy()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the label and features\n",
    "        label = self.data[idx, 0]\n",
    "        features = self.data[idx, 1:] / 255\n",
    "        # print(features.shape)\n",
    "\n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = myDataset('data/fashionmnist_train.csv')\n",
    "dataset_test = myDataset('data/fashionmnist_test.csv')\n",
    "\n",
    "loader_train = DataLoader(dataset_train, batch_size=32, shuffle=True)\n",
    "loader_test = DataLoader(dataset_test, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in loader_train:\n",
    "    print(x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_train), len(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # Define our layers\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "        self.to(device=device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward pass of our model\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def Train(self, epochs, optimizer, loader_train, loader_test, verbose=True):\n",
    "        \n",
    "        self.loss_train_log = []\n",
    "        self.loss_test_log = []\n",
    "        self.best_loss = np.inf\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.train() # need to specify as certain layers (e.g. dropout) behave differently in train/eval\n",
    "            \n",
    "            # in every epoch, we will:\n",
    "            # (1) loop over loader_train to train the model\n",
    "            # (2) calculate the loss of training data and save it (for loss curve)\n",
    "            # (3) calculate the loss of val/testing data and save it (for loss curve)\n",
    "            # (4) print training progress\n",
    "            # (5) early stopping\n",
    "\n",
    "            # Step (1)        \n",
    "            for x, y in loader_train:\n",
    "                x = x.to(device=self.device, dtype=torch.float)\n",
    "                y = y.to(device=self.device, dtype=torch.long) # label is torch.float if regression, torch.long if classification\n",
    "\n",
    "                # Reset the gradients\n",
    "                optimizer.zero_grad() # SOLUTION\n",
    "\n",
    "                # Forward pass and calculate loss\n",
    "                yhat = self.forward(x) # SOLUTION\n",
    "                loss = F.cross_entropy(yhat, y) # SOLUTION\n",
    "\n",
    "                # Backward pass and update weights\n",
    "                loss.backward() # SOLUTION\n",
    "                optimizer.step() # SOLUTION\n",
    "\n",
    "            # Step (2) (need to complete the self.evaluate function to work)\n",
    "            loss_train = self.evaluate(loader_train) # SOLUTION\n",
    "            self.loss_train_log.append(loss_train) # SOLUTION\n",
    "\n",
    "            # Step (3) (need to complete the self.evaluate function to work)\n",
    "            loss_test = self.evaluate(loader_test) # SOLUTION\n",
    "            self.loss_test_log.append(loss_test) # SOLUTION\n",
    "\n",
    "            # Step (4)\n",
    "            if verbose:\n",
    "                print('Epochs %d/%d' % (epoch+1, epochs))\n",
    "                print('Train Loss = %.4f' % loss_train, end=', ')\n",
    "                print('Val Loss = %.4f' % loss_test)\n",
    "\n",
    "            # Step (5)\n",
    "            if loss_test < self.best_loss:\n",
    "                self.best_loss = loss_test\n",
    "                best_epoch = epoch + 1\n",
    "                torch.save(self.state_dict(), 'best_params.pt')\n",
    "        \n",
    "        print(f'Best model saved at epoch {best_epoch} with loss {self.best_loss:.4f}.')\n",
    "\n",
    "    def evaluate(self, loader):\n",
    "        # this function is to evaluate the model on a given dataset (loader) by computing the average loss\n",
    "\n",
    "        self.eval() # need to specify as certain layers (e.g. dropout) behave differently in train/eval\n",
    "        \n",
    "        loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in loader:\n",
    "                x = x.to(device=self.device, dtype=torch.float)\n",
    "                y = y.to(device=self.device, dtype=torch.long) # label is torch.float if regression, torch.long if classification\n",
    "\n",
    "                # forward pass and calculate loss\n",
    "                yhat = self.forward(x) # SOLUTION\n",
    "                loss += F.cross_entropy(yhat, y, reduction='sum') # SOLUTION\n",
    "        \n",
    "        loss /= len(loader.dataset)\n",
    "        return loss.cpu()\n",
    "    \n",
    "    def predict(self, loader):\n",
    "        # this function is to provide the model's prediction on a given dataset (loader).\n",
    "        # it returns the prediction, together with the corresponding input x and label y (for evaluation/visualization purposes)\n",
    "        \n",
    "        self.eval() # need to specify as certain layers (e.g. dropout) behave differently in train/eval\n",
    "\n",
    "         # SOLUTION\n",
    "        x_all, y_all, pred = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y in loader:\n",
    "                x_all.append(x)\n",
    "                y_all.append(y)\n",
    "\n",
    "                x = x.to(device=self.device, dtype=torch.float)\n",
    "                yhat = self.forward(x) # SOLUTION\n",
    "                pred.append(yhat.cpu())\n",
    "                \n",
    "            x_all, y_all, pred = torch.cat(x_all), torch.cat(y_all), torch.cat(pred)\n",
    "            return x_all, y_all, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T1.3 IMPORTANT: Please do not edit/remove this comment.\n",
    "\n",
    "lr = 0.01\n",
    "epochs = 20\n",
    "model = MLP(device=device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "model.Train(epochs, optimizer, loader_train, loader_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Inspecting the loss\n",
    "\n",
    "In the model class, there are two variables `self.loss_train_log` and `self.loss_test_log` which record the historical losses as the training progresses. Plot both the losses in the same figure to visualize the training progress, and ensure that proper labels and legend are in-place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T1.4a IMPORTANT: Please do not edit/remove this comment.\n",
    "\n",
    "# Plot the losses in the same figure\n",
    "plt.plot(model.loss_train_log)\n",
    "plt.plot(model.loss_test_log)\n",
    "plt.legend(['train loss', 'test loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T1.4b IMPORTANT: Please do not edit/remove this comment.\n",
    "\n",
    "x_all, y_all, pred = model.predict(loader_test)\n",
    "(F.softmax(pred, dim=1).argmax(dim=1) == y_all).sum()/len(y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = torch.load('best_params.pt')\n",
    "model_best = MLP(device=device)\n",
    "model_best.load_state_dict(best_params)\n",
    "x_all, y_all, pred = model_best.predict(loader_test)\n",
    "(F.softmax(pred, dim=1).argmax(dim=1) == y_all).sum()/len(y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Compose function allows you to combine multiple image transformations into a sequential pipeline. \n",
    "# Each transformation will be applied in the order they appear within the list.\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomHorizontalFlip(), # Randomly flip images horizontally (left to right)\n",
    "    transforms.RandomRotation(10), # Apply a random rotation of +-10 degrees to each image\n",
    "    transforms.ToTensor(), # Convert the image to a PyTorch tensor in the range of [0,1]\n",
    "    transforms.Normalize(mean=[0.5,], std=[0.5,]), # Normalise the image tensor with mean and standard deviation values of 0.5 for each channel\n",
    "])\n",
    "\n",
    "# Augmentation is not applied to test data\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(), # Convert the image to a PyTorch tensor in the range of [0,1]\n",
    "    transforms.Normalize(mean=[0.5,], std=[0.5,]), # Normalise the image tensor with mean and standard deviation values of 0.5 for each channel\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNISTDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data = pd.read_csv(csv_file).to_numpy()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the label and features\n",
    "        label = self.data[idx, 0]\n",
    "        image = self.data[idx, 1:].reshape(28, 28, 1) / 255\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = FashionMNISTDataset('data/fashion-mnist_train.csv', transform=transform_train)\n",
    "dataset_test = FashionMNISTDataset('data/fashion-mnist_test.csv', transform=transform_test)\n",
    "\n",
    "loader_train = DataLoader(dataset_train, batch_size=32, shuffle=True)\n",
    "loader_test = DataLoader(dataset_test, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # First convolutional block\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Second convolutional block\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Third convolutional block\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        # self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.pool3 = nn.AvgPool2d(kernel_size=7)\n",
    "\n",
    "        # Fully connected layer(s)\n",
    "        self.fc = nn.Linear(64 * 1 * 1, 10)  # After 3 pooling layers on 28x28 input, spatial dims are 3x3\n",
    "\n",
    "        self.to(device=device)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Conv Block 1\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        # Conv Block 2\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # Conv Block 3\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        # Flatten for FC layers\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def Train(self, epochs, optimizer, loader_train, loader_test, verbose=True):\n",
    "        self.loss_train_log = []\n",
    "        self.loss_test_log = []\n",
    "        best_loss = np.inf\n",
    "        for epoch in range(epochs):\n",
    "            self.train() # need to specify as certain layers (e.g. dropout) behave differently in train/eval\n",
    "            \n",
    "            # in every epoch, we will:\n",
    "            # (1) loop over loader_train to train the model\n",
    "            # (2) calculate the loss of training data and save it (for loss curve)\n",
    "            # (3) calculate the loss of val/testing data and save it (for loss curve)\n",
    "            # (4) print training progress\n",
    "            # (5) early stopping\n",
    "\n",
    "            # Step (1)        \n",
    "            for x, y in loader_train:\n",
    "                x = x.to(device=self.device, dtype=torch.float)\n",
    "                y = y.to(device=self.device, dtype=torch.long) # label is torch.float if regression, torch.long if classification\n",
    "\n",
    "                # Reset the gradients\n",
    "                optimizer.zero_grad() # SOLUTION\n",
    "\n",
    "                # Forward pass and calculate loss\n",
    "                yhat = self.forward(x) # SOLUTION\n",
    "                loss = F.cross_entropy(yhat, y) # SOLUTION\n",
    "\n",
    "                # Backward pass and update weights\n",
    "                loss.backward() # SOLUTION\n",
    "                optimizer.step() # SOLUTION\n",
    "\n",
    "            # Step (2) (need to complete the self.evaluate function to work)\n",
    "            loss_train = self.evaluate(loader_train) # SOLUTION\n",
    "            self.loss_train_log.append(loss_train) # SOLUTION\n",
    "\n",
    "            # Step (3) (need to complete the self.evaluate function to work)\n",
    "            loss_test = self.evaluate(loader_test) # SOLUTION\n",
    "            self.loss_test_log.append(loss_test) # SOLUTION\n",
    "\n",
    "            # Step (4)\n",
    "            if verbose:\n",
    "                print('Epochs %d/%d' % (epoch+1, epochs))\n",
    "                print('Train Loss = %.4f' % loss_train, end=', ')\n",
    "                print('Val Loss = %.4f' % loss_test)\n",
    "\n",
    "            # Step (5)\n",
    "            if loss_test < best_loss:\n",
    "                best_loss = loss_test\n",
    "                best_epoch = epoch + 1\n",
    "                torch.save(self.state_dict(), 'best_params.pt')\n",
    "        \n",
    "        print(f'Best model saved at epoch {best_epoch} with loss {best_loss:.4f}.')\n",
    "\n",
    "    def evaluate(self, loader):\n",
    "        # this function is to evaluate the model on a given dataset (loader) by computing the average loss\n",
    "\n",
    "        self.eval() # need to specify as certain layers (e.g. dropout) behave differently in train/eval\n",
    "        \n",
    "        loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in loader:\n",
    "                x = x.to(device=self.device, dtype=torch.float)\n",
    "                y = y.to(device=self.device, dtype=torch.long) # label is torch.float if regression, torch.long if classification\n",
    "\n",
    "                # forward pass and calculate loss\n",
    "                yhat = self.forward(x) # SOLUTION\n",
    "                loss += F.cross_entropy(yhat, y, reduction='sum') # SOLUTION\n",
    "        \n",
    "        loss /= len(loader.dataset)\n",
    "        return loss.cpu()\n",
    "    \n",
    "    def predict(self, loader):\n",
    "        # this function is to provide the model's prediction (and the corresponding ground truth) on a given dataset (loader).\n",
    "        # both actual and pred should be arrays of shape (m,1) where m is the number of samples.\n",
    "        \n",
    "        self.eval() # need to specify as certain layers (e.g. dropout) behave differently in train/eval\n",
    "\n",
    "         # SOLUTION\n",
    "        x_all, y_all, pred = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y in loader:\n",
    "                x_all.append(x)\n",
    "                y_all.append(y)\n",
    "\n",
    "                x = x.to(device=self.device, dtype=torch.float)\n",
    "                yhat = self.forward(x) # SOLUTION\n",
    "                pred.append(yhat.cpu())\n",
    "            \n",
    "            x_all, y_all, pred = torch.cat(x_all), torch.cat(y_all), torch.cat(pred)\n",
    "            return x_all, y_all, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr = 0.01\n",
    "epochs = 20\n",
    "model = CNN(device=device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "model.Train(epochs, optimizer, loader_train, loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T1.4a IMPORTANT: Please do not edit/remove this comment.\n",
    "\n",
    "# Plot the losses in the same figure\n",
    "plt.plot(model.loss_train_log)\n",
    "plt.plot(model.loss_test_log)\n",
    "plt.legend(['train loss', 'test loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T1.4b IMPORTANT: Please do not edit/remove this comment.\n",
    "\n",
    "x_all, y_all, pred = model.predict(loader_test)\n",
    "(F.softmax(pred, dim=1).argmax(dim=1) == y_all).sum()/len(y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = torch.load('best_params.pt')\n",
    "model_best = CNN()\n",
    "model_best.load_state_dict(best_params)\n",
    "x_all, y_all, pred = model_best.predict(loader_test)\n",
    "(F.softmax(pred, dim=1).argmax(dim=1) == y_all).sum()/len(y_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
