{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "# <p style=\"text-align: center;\">Lab 3: Multi-Layer Perceptrons (MLP) </p>\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"data/thumbnail.png\" width=\"544\" height=\"300\" />\n",
    "</div>\n",
    "\n",
    "Welcome to your third lab of ECE4179! Labs in this unit will run as a help desk and they are not mandatory to attend.\n",
    "\n",
    "This notebook contains all the code and comments that you need to submit. Here are the instructions (same as Lab 1) to complete this lab:\n",
    "\n",
    "- Your grade is entirely based on notebook completion (no quiz).\n",
    "- After completing the notebook, submit it to Moodle under '**Lab 3 Submission**'.\n",
    "- Before submission, make sure all outputs are visible by running all your cells from top to bottom (you can click _Run All_ at the top).\n",
    "- IMPORTANT: Some parts of the notebook will be auto-graded, therefore please do not edit/rename the already-given variable/function/class names.\n",
    "\n",
    "This lab has two tasks. These tasks will require you to use a deep learning library (Pytorch) for MLP-based problems. These knowledge and skills will be essential for Lab 4 and assignment, and in general, critical to get you prepared to enter the deep learning world.\n",
    "\n",
    "- [Task 1: Shallow MLP to a sinusoidal function](#task1)\n",
    "- [Task 2: Compare Shallow MLP and Deep MLP](#task2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "### Pytorch\n",
    "\n",
    "PyTorch is based on the Torch library, adapted for Python. PyTorch is a deep learning library that allows you to have greater control of your neural network architecture, and have a lot more customisable hyper parameters / functions (such as the loss function etc.). Learning PyTorch not only provides a strong foundation in deep learning but also makes it easier to transition to other popular libraries like TensorFlow and Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "device = 'cpu' # this lab does not need gpu/cuda\n",
    "\n",
    "def seed_all(seed=0):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## Task 1 - Approximate the sine function <a class=\"anchor\" id=\"task1\"></a>\n",
    "\n",
    "In this section, we will approximate the sine function with a neural network to get a sense of how architecture and hyperparameters affect neural network performance.\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"data/sine.gif\" width=\"500\" />\n",
    "</div>\n",
    "\n",
    "### Learning Outcome\n",
    "- Define the custom dataset class (i.e., Train and Test datasets) and visualize the train dataset you've created.\n",
    "- Design the Shallow Linear MLP model using PyTorch.\n",
    "- Train and evaluate the MLP model on defined train dataset and test dataset.\n",
    "- Visualize experimental results using Matplotlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### 1.1 Create custom dataset and dataloaders <a class=\"anchor\" id=\"1_1\"></a>\n",
    "\n",
    "Pytorch has a huge number of functionalities that make training our neural networks very easy! One of those functionalities is the Pytorch dataset and dataloader (they are real life-savers!). In depth review on PyTorch Datasets and Dataloaders are covered in Lectures!\n",
    "\n",
    "\n",
    "In PyTorch, there are two Classes that can make the training process much easier - **Dataset** ([```torch.utils.data.Dataset```](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)) and **Dataloader** ([```torch.utils.data.DataLoader```](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)). Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples. Take a look that [this page](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) that explain further about data handling in PyTorch.\n",
    "\n",
    "Simply speaking, a **Dataset** object loads the raw data and provides easy access to each sample (sample point + their label). Then a **Dataloader** object takes the Dataset object and creates batches of the data for model training/testing.\n",
    "\n",
    "In the following section, let's create our Dataset and Dataloader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Creating a dataset\n",
    "\n",
    "The dataset you are going to be creating will be points from a \"noisy\" sine wave. To create the custom dataset, you can use PyTorch dataset inbuilt functionalities. <br>\n",
    "\n",
    "\n",
    "The Pytorch dataset class has three essential parts:<br>\n",
    "1. The \\__init__ function (as most Python classes do)<br>\n",
    "2. The \\__getitem__ function (this is called during every iteration)<br>\n",
    "3. The \\__len__ function (this must return the length of the dataset)\n",
    "\n",
    "**Remember! The \"self\" within classes will become attributes of that class that you can use within other methods that are defined for that class. If you defined an attribute without self.<\\name>, then that attribute cannot be used for other methods.**\n",
    "\n",
    "Make sure to follow all inline comments specified for each code fragment that you need to work on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T1.1a IMPORTANT: Please do not edit/remove this comment.\n",
    "\n",
    "# Prepare the Dataset class which stores our data and label.\n",
    "# In real application, dataset is usually located in folders and need to be loaded into the class (we will do this in next task).\n",
    "# In this task we will start simple, by artificially generating our data and label.\n",
    "class SineDataset(Dataset):\n",
    "    \"\"\" Data noisy sinewave dataset\n",
    "        num_samples - the number of samples you want\n",
    "        bound - the upper and lower bounds of the samples\n",
    "    \"\"\"\n",
    "    def __init__(self, num_samples, bound):\n",
    "        # This init function is where we load and store our dataset\n",
    "        # In this task, our data is artificial (generated noisy sinewave points)\n",
    "        \n",
    "        # Create \"num_samples\" worth of random x points between -bound and +bound\n",
    "        # by using torch.rand, then scale and shift\n",
    "        self.x_data = \n",
    "        \n",
    "        # Calculate the sin of all data points in the x vector\n",
    "        # and then scale amplitude to 0.5\n",
    "        self.y_data = ???\n",
    "        \n",
    "        # Add some gaussian noise to each datapoint using torch.randn_like\n",
    "        # The noise needs to be scaled down (use a factor of 0.08) before added to the signal,\n",
    "        # otherwise it will be too noisy\n",
    "        self.y_data += ???\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # This function returns the idx-th data of our already-loaded data (self.x_data and self.y_data)\n",
    "        # This function will be called by DataLoader class whenever the latter wants a sample (x-y pair)\n",
    "        # It is up to this function to prepare the (x-y pairs) for the DataLoader\n",
    "        # If DataLoader wants a mini-batch, it will call this function repeatedly\n",
    "        # DataLoader will also convert numpy array to tensor array, if not already in tensor\n",
    "        \n",
    "        # IMPORTANT: The indexed data (both x and y) should have 1-dimensional shape only,\n",
    "        # i.e. its feature size. In this case, there is only 1 feature, so shape should be (1,) or torch.Size([1])\n",
    "        x = ???\n",
    "        y = ???\n",
    "        sample = (x, y)\n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        # This function returns the total number of samples in our dataset\n",
    "        # DataLoader will use this function to determine how many mini-batches to prepare.\n",
    "        num_samples = ???\n",
    "        return num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Create Dataset instance from the defined Class\n",
    "\n",
    "Now that you've defined your dataset Class, lets create an instance of it for training and testing and then create dataloaders to make it easy to iterate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T1.1b IMPORTANT: Please do not edit/remove this comment.\n",
    "\n",
    "num_train = 5000    # the number of training datapoints\n",
    "num_test = 1000     # the number of testing datapoints\n",
    "bound = 5           # we will create a sine wave between -5 and +5\n",
    "bs_train = 32       # the batch size for training task 1\n",
    "bs_test = 1         # the batch size for testing task 1\n",
    "\n",
    "# Create an instance of the SineDataset for both the training and test set \n",
    "# For this task we will not use a validation set\n",
    "dataset_train = ???\n",
    "dataset_test  = ???\n",
    "\n",
    "# Create dataloaders for both train and test by passing some arguments\n",
    "# argument 1: the Dataset instance\n",
    "# argument 2: batch size\n",
    "# argument 3: shuffle (True for train, False for test)\n",
    "loader_train = ???\n",
    "loader_test = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Visualise the dataset you've created!\n",
    "\n",
    "Using scatter plot, visualize the train and test dataset in a 1 by 2 subplot. Access the data via the Dataset class instance that you have created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T1.1c IMPORTANT: Please do not edit/remove this comment.\n",
    "\n",
    "# Create the scatter plot using just the test data\n",
    "# You can make the marker size smaller for better visibility\n",
    "\n",
    "plt.figure(figsize=(10,5)) # adjust as needed\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### 1.2 Design a Neural Network <a class=\"anchor\" id=\"1_2\"></a>\n",
    "\n",
    "In this task, we will build a *Shallow MLP* that can be trained to approximate a sine wave.\n",
    "\n",
    "\n",
    "#### Activation functions\n",
    "\n",
    "You will start by creating a MLP—a neural network with one or many \"hidden\" layers separated by \"**activation functions**\" that give our network \"**non-linearities**\". If we don't have these activation functions and simply stacked layers together, our network would be no better than a single linear layer! (_Why?_)\n",
    "\n",
    "In a MLP, only the output layer’s behavior is explicitly guided by the training data, as we compute a loss function based on its output and the corresponding label. In other words, the output layer produces values that are meaningful to us (i.e. the prediction). In contrast, the hidden layers do not have direct supervision—our training data do not specify what their outputs should be. As a result, the output values of hidden layers are implicitly learned by the algorithm, and do not have a clear, predefined physical meaning (hence the name \"hidden\"). This lack of interpretability makes trained neural networks resemble a \"black box\"—a complex system whose internal workings are hidden or not readily understood.\n",
    "\n",
    "Each hidden unit receives inputs from multiple other units and computes its own activation value based on a chosen activation function. Our role is to select appropriate activation functions, while the learning process determines how the hidden layers contribute to achieving the final output. So what are these nonlinear activation functions that turn our simple linear models into a power \"nonlinear function approximator\"? Some common examples are:<br>\n",
    "1. relu\n",
    "2. sigmoid\n",
    "3. tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's design a two layer Shallow MLP model with **one hidden layer** and **tanh** activation function using PyTorch. The model structure is given as follows:\n",
    "\n",
    "fc1 : Linear(1 $\\times$ d) $\\rightarrow$ tanh $\\rightarrow$ fc2 : Linear(d $\\times$ 1)\n",
    "\n",
    "Here since both the input data *x* and target value *y* are just one number, the input and output of the network are 1. And **d** is the dimension we define for the hidden layer.\n",
    "\n",
    "We are going to use the **mean square error loss** for our model training - [```nn.MSELoss()```](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html).\n",
    "\n",
    "There are two basic functions we first need to define: \\_\\_init__() and forward().\n",
    "\n",
    "- In \\_\\_init__(), we define the structure/architecture of our model. Similar to all python classess, this function will be called when a new instance is made.\n",
    "\n",
    "- In forward(), we define the forward propagation for the model. This method is called by the object to do forward pass.\n",
    "\n",
    "Other core functions include:\n",
    "\n",
    "- Train() - training procedures for our model.\n",
    "\n",
    "- evaluate() - validation/testing procedures for our model that returns the loss.\n",
    "\n",
    "- predict() - prediction procedures for our model that returns the ground truth and the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T1.2 IMPORTANT: Please do not edit/remove this comment.\n",
    "\n",
    "class ShallowMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # Define our layers\n",
    "        ???\n",
    "\n",
    "        self.to(device=device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward pass of our model\n",
    "        ???\n",
    "        return ???\n",
    "    \n",
    "    def Train(self, epochs, optimizer, loader_train, loader_test, verbose=True):\n",
    "        self.loss_train_log = []\n",
    "        self.loss_test_log = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.train() # need to specify as certain layers (e.g. dropout) behave differently in train/eval\n",
    "            \n",
    "            # in every epoch, we will:\n",
    "            # (1) loop over loader_train to train the model\n",
    "            # (2) calculate the loss of training data and save it (for loss curve)\n",
    "            # (3) calculate the loss of val/testing data and save it (for loss curve)\n",
    "            # (4) print training progress\n",
    "            # don't need early stopping for this lab\n",
    "\n",
    "            # Step (1)        \n",
    "            for x, y in loader_train:\n",
    "                x = x.to(device=self.device, dtype=torch.float)\n",
    "                y = y.to(device=self.device, dtype=torch.float) # label is torch.float if regression, torch.long if classification\n",
    "\n",
    "                # Reset the gradients\n",
    "                ???\n",
    "\n",
    "                # Forward pass and calculate loss\n",
    "                ???\n",
    "\n",
    "                # Backward pass and update weights\n",
    "                ???\n",
    "\n",
    "            # Step (2) (need to complete the self.evaluate function to work)\n",
    "            ???\n",
    "\n",
    "            # Step (3) (need to complete the self.evaluate function to work)\n",
    "            ???\n",
    "\n",
    "            # Step (4)\n",
    "            if verbose:\n",
    "                ???\n",
    "\n",
    "\n",
    "    def evaluate(self, loader):\n",
    "        # this function is to evaluate the model on a given dataset (loader) by computing the average loss\n",
    "\n",
    "        self.eval() # need to specify as certain layers (e.g. dropout) behave differently in train/eval\n",
    "        \n",
    "        loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in loader:\n",
    "                x = x.to(device=self.device, dtype=torch.float)\n",
    "                y = y.to(device=self.device, dtype=torch.float) # label is torch.float if regression, torch.long if classification\n",
    "\n",
    "                # forward pass and calculate loss\n",
    "                ???\n",
    "        \n",
    "        ???\n",
    "        return loss.cpu()\n",
    "    \n",
    "    def predict(self, loader):\n",
    "        # this function is to provide the model's prediction (and the corresponding ground truth) on a given dataset (loader).\n",
    "        # both actual and pred should be arrays of shape (m,1) where m is the number of samples.\n",
    "\n",
    "        self.eval() # need to specify as certain layers (e.g. dropout) behave differently in train/eval\n",
    "\n",
    "        x_all, y_all, pred = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y in loader:\n",
    "                x_all.append(x)\n",
    "                y_all.append(y)\n",
    "                \n",
    "                x = x.to(device=self.device, dtype=torch.float)\n",
    "\n",
    "                # forward pass and store predictions\n",
    "                ???\n",
    "            \n",
    "            ???\n",
    "            return x_all, y_all, pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### 1.3 Train and evaluate the network  <a class=\"anchor\" id=\"1_3\"></a>\n",
    "\n",
    "Now let's do some training for the network we just created! In this task, train our model using the following hyperparameters:\n",
    "- Learning rate = 5e-3 (same as 0.005)\n",
    "- Number of epochs = 10\n",
    "- Hidden layer size = 16\n",
    "- Optimizer = Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T1.3 IMPORTANT: Please do not edit/remove this comment.\n",
    "\n",
    "lr = ???\n",
    "epochs = ???\n",
    "model = ???\n",
    "optimizer = ???\n",
    "model.Train(epochs, optimizer, loader_train, loader_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### 1.4 Visualise and Analyse the Experimental Results <a class=\"anchor\" id=\"1_4\"></a>\n",
    "\n",
    "Now the training is done. Let's check how well our model has performed.\n",
    "\n",
    "There are a few ways we can evaluate the model performance:\n",
    "- Evaluate accuracy\n",
    "- Inspect the loss\n",
    "- Precision, recall and F1-Score\n",
    "- Confusion Matrix\n",
    "And more\n",
    "\n",
    "Let's try some of them here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Inspecting the loss\n",
    "\n",
    "In the model class, there are two variables `self.loss_train_log` and `self.loss_test_log` which record the historical losses as the training progresses. Plot both the losses in the same figure to visualize the training progress, and ensure that proper labels and legend are in-place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T1.4a IMPORTANT: Please do not edit/remove this comment.\n",
    "\n",
    "# Plot the losses in the same figure\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Visual check on the model predictions\n",
    "\n",
    "Observe the performance of the trained model by visualising the ground truth and the prediction.\n",
    "- Pass the test dataloader into the `predict` function of model class to obtain the ground truth and the prediction.\n",
    "- Plot them in the same figure using scatter plot, with proper labels and legend.\n",
    "- The plot should be similar to the Task 1 thumbnail:\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"data/sine.jpg\" width=\"300\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T1.4b IMPORTANT: Please do not edit/remove this comment.\n",
    "\n",
    "??? = model.???\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## Task 2: Compare Shallow MLP and Deep MLP <a class=\"anchor\" id=\"task2\"></a>\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"data/mlp.png\" width=\"800\" />\n",
    "</div>\n",
    "\n",
    "In this task, you will be training a Shallow MLP and Deep MLP to predict the compressive strength of concrete. The details of the dataset are outlined as follows:\n",
    "\n",
    "- The dataset consists of 8 input features which are used to predict the concrete compressive strength (output)\n",
    "- There are a total of 1030 datapoints. In the `data` folder, you will find `concrete_train.csv` and `concrete_test.csv` containing 827 training data and 203 testing data respectively.\n",
    "- The following table describes each column entry in the csv file:\n",
    "\n",
    "| **Column Name**                | **Units**     | **Description**  |\n",
    "|--------------------------------|---------------|------------------|\n",
    "| Cement                         | kg/m^3   | Feature 1             |\n",
    "| Blast Furnace Slag             | kg/m^3   | Feature 2             |\n",
    "| Fly Ash                        | kg/m^3   | Feature 3             |\n",
    "| Water                          | kg/m^3   | Feature 4             |\n",
    "| Superplasticizer               | kg/m^3   | Feature 5             |\n",
    "| Coarse Aggregate\t             | kg/m^3   | Feature 6             |\n",
    "| Fine Aggregate\t             | kg/m^3   | Feature 7             |\n",
    "| Age                            | day      | Feature 8             |\n",
    "| Concrete compressive strength\t | MPa      | Target                |\n",
    "\n",
    "### Learning Outcome\n",
    "- Preprocess the Concrete Compressive Strength dataset and create the Dataset and DataLoader for it.\n",
    "- Perform training of model using a shallow feedforward neural network\n",
    "- Design a deep feedforward neural network using Pytorch and train the model\n",
    "- Visualize and analyse the performance of both shallow and deep MLP in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code loads the data for this task and stores it as numpy array. Do not modify\n",
    "data_train = pd.read_csv('data/concrete_train.csv').to_numpy()\n",
    "data_test = pd.read_csv('data/concrete_test.csv').to_numpy()\n",
    "print(f'Shape of train data: {data_train.shape}')\n",
    "print(f'Shape of test data: {data_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### 2.1 Preprocessing of input features\n",
    "\n",
    "Before we use the data for training, we need to preprocess the data first. If you open and check the csv files, you will see that the features and labels have different range of values. To train deep learning models, it is important to ensure that the values have similar scales for better convergence.\n",
    "\n",
    "You are required to:\n",
    "- Perform standardization (minus mean and divide by standard deviation)\n",
    "- This needs to be done for every feature and the label (i.e. across the columns)\n",
    "- In other words, for each column, we find the mean and std, then use these parameters to standardize each value in that column\n",
    "- If done correctly, each column should have mean close to 0 and standard deviation close to 1 (You can use this fact to check)\n",
    "- IMPORTANT: To standardize test data, you need to use the mean and std values from the TRAIN data !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T2.1 IMPORTANT: Please do not edit/remove this comment.\n",
    "\n",
    "# Find the parameters (mean and std) using np.mean and np.std function.\n",
    "# Hint: Use the \"axis\" argument in the functions to find mean/std across columns\n",
    "means = ???\n",
    "devs = ???\n",
    "\n",
    "# Standardize train data\n",
    "data_train = ???\n",
    "\n",
    "# Standardize test data\n",
    "data_test = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### 2.2 Train a Shallow MLP\n",
    "\n",
    "This task is similar to Task 1. We will:\n",
    "- (a) create the custom Dataset\n",
    "- (b) instantiate the class and create the dataloaders\n",
    "- (c) reuse the ShallowMLP class defined in Task 1 to train a regression model for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T2.2a IMPORTANT: Please do not edit/remove this comment.\n",
    "\n",
    "# You will follow the same process as you did in the first task: Dataset > DataLoader > Model\n",
    "class ConcreteDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        # We can directly pass the standardized data into the class here.\n",
    "        \n",
    "        # The task is simple here: to seperate the features (first 8 columns) and the label (last column)\n",
    "        self.x_data = ???\n",
    "        self.y_data = ???\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # This function returns the idx-th data of our already-loaded data (self.x_data and self.y_data)\n",
    "        # This function will be called by DataLoader class whenever the latter wants a sample (x-y pair)\n",
    "        # It is up to this function to prepare the (x-y pairs) for the DataLoader\n",
    "        # If DataLoader wants a mini-batch, it will call this function repeatedly\n",
    "        # IMPORTANT: The indexed data should have 1-dimensional shape only.\n",
    "        x = ??? # make sure the shape is correct\n",
    "        y = ??? # make sure the shape is correct\n",
    "        sample = (x, y)\n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        # This function returns the total number of samples in our dataset\n",
    "        # DataLoader will use this function to determine how many mini-batches to prepare.\n",
    "        num_samples = ???\n",
    "        return num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T2.2b IMPORTANT: Please do not edit/remove this comment.\n",
    "\n",
    "bs_train = 32       # the batch size for training task 2\n",
    "bs_test = 1         # the batch size for testing task 2\n",
    "\n",
    "# Create an instance of the ConcreteDataset for both the training and test set \n",
    "dataset_train = ???\n",
    "dataset_test  = ???\n",
    "\n",
    "# Create dataloaders for both train and test\n",
    "loader_train = ???\n",
    "loader_test = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T2.2c IMPORTANT: Please do not edit/remove this comment.\n",
    "\n",
    "# Train the model here\n",
    "# Use a learning rate of 1e-3, train for a total of 50 epochs, and set the hidden layer size to be 64\n",
    "# Use the Adam optimizer\n",
    "lr = ???\n",
    "epochs = ???\n",
    "model_shallow = ???\n",
    "optimizer = ???\n",
    "model_shallow.Train(epochs, optimizer, loader_train, loader_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### 2.3 Design and train a deep MLP\n",
    "\n",
    "Here, we will create a new model class called DeepMLP. The difference between this class and our previous ShallowMLP class is only the structure/architecture of the model. Therefore, only the **init()** function and the **forward()** functions require modification.\n",
    "\n",
    "You are required to create the model structure as follows:\n",
    "- fc1 : Linear(input_size $\\times$ hidden_size)\n",
    "- fc2 : Linear(hidden_size $\\times$ 64)\n",
    "- fc3 : Linear(64 $\\times$ 32)\n",
    "- fc4 : Linear(32 $\\times$ 16)\n",
    "- fc5 : Linear(16 $\\times$ output_size)\n",
    "\n",
    "**Between** the linear layers, make sure to include the use of activation function. For this task, use the **ReLU** activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T2.3a IMPORTANT: Please do not edit/remove this comment.\n",
    "\n",
    "class DeepMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # Define our layers\n",
    "        ???\n",
    "\n",
    "        self.to(device=device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward pass of our model\n",
    "        ???\n",
    "        return ???\n",
    "    \n",
    "    def Train(self, epochs, optimizer, loader_train, loader_test, verbose=True):\n",
    "        self.loss_train_log = []\n",
    "        self.loss_test_log = []\n",
    "        for epoch in range(epochs):\n",
    "            self.train() # need to specify as certain layers (e.g. dropout) behave differently in train/eval\n",
    "            \n",
    "            # in every epoch, we will do:\n",
    "            # (1) loop over loader_train to train the model\n",
    "            # (2) calculate the loss of training data and save it (for loss curve)\n",
    "            # (3) calculate the loss of val/testing data and save it (for loss curve)\n",
    "            # (4) print training progress\n",
    "            # don't need early stopping for this lab\n",
    "\n",
    "            # Step (1)        \n",
    "            for x, y in loader_train:\n",
    "                x = x.to(device=self.device, dtype=torch.float)\n",
    "                y = y.to(device=self.device, dtype=torch.float) # label is torch.float if regression, torch.long if classification\n",
    "\n",
    "                # Reset the gradients\n",
    "                ???\n",
    "\n",
    "                # Forward pass and calculate loss\n",
    "                ???\n",
    "\n",
    "                # Backward pass and update weights\n",
    "                ???\n",
    "\n",
    "            # Step (2) (need to complete the self.evaluate function to work)\n",
    "            ???\n",
    "\n",
    "            # Step (3) (need to complete the self.evaluate function to work)\n",
    "            ???\n",
    "\n",
    "            # Step (4)\n",
    "            if verbose:\n",
    "                ???\n",
    "\n",
    "    def evaluate(self, loader):\n",
    "        self.eval() # need to specify as certain layers (e.g. dropout) behave differently in train/eval\n",
    "        \n",
    "        loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in loader:\n",
    "                x = x.to(device=self.device, dtype=torch.float)\n",
    "                y = y.to(device=self.device, dtype=torch.float) # label is torch.float if regression, torch.long if classification\n",
    "\n",
    "                # forward pass and calculate loss\n",
    "                ???\n",
    "        \n",
    "        ???\n",
    "        return loss.cpu()\n",
    "    \n",
    "    def predict(self, loader):\n",
    "        self.eval() # need to specify as certain layers (e.g. dropout) behave differently in train/eval\n",
    "\n",
    "        x_all, y_all, pred = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y in loader:\n",
    "                x_all.append(x)\n",
    "                y_all.append(y)\n",
    "                \n",
    "                x = x.to(device=self.device, dtype=torch.float)\n",
    "\n",
    "                # forward pass and store predictions\n",
    "                ???\n",
    "            \n",
    "            ???\n",
    "            return x_all, y_all, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T2.3b IMPORTANT: Please do not edit/remove this comment.\n",
    "\n",
    "# Train the deep MLP model. Use the same hyperparameters as the previous one\n",
    "# learning rate = 1e-3, epochs = 50, hidden size = 64, Adam optimizer\n",
    "lr = ???\n",
    "epochs = ???\n",
    "model_deep = ???\n",
    "optimizer = ???\n",
    "model_deep.Train(epochs, optimizer, loader_train, loader_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### 2.4 Visualise and Analyse the Experimental Results\n",
    "\n",
    "Let's check and compare the performance of our shallow and deep model. Similar to Task 1, we will inspect the loss as well as visualize the predictions using matplotlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Inspecting the loss\n",
    "\n",
    "For both model_shallow and model_deep, they have their respective train and test losses. Therefore, there are a total of 4 loss curves here. In order to compare the two models, plot all 4 loss curves **in the same figure**. Ensure that proper labels and legend are in-place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T2.4a IMPORTANT: Please do not edit/remove this comment.\n",
    "\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Visual check on the model predictions\n",
    "\n",
    "Observe the performance of the trained model by visualising the ground truth and the prediction.\n",
    "- Pass the test dataloader into the `predict` function of model_shallow to obtain the ground truth and the prediction.\n",
    "- For the plot, it will be slightly different from Task 1 - you are required to do a scatter plot, with the ground truth as the x-axis, and prediction as the y-axis.\n",
    "- The plot should look something like this:\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"data/r2plot.jpg\" width=\"200\" />\n",
    "</div>\n",
    "\n",
    "- Plotting them this way is convenient to judge the performance. A perfect straight line indicates perfect prediction.\n",
    "- Calculate the coefficient of determination (R2) for the model. The function `r2_score` has already been imported for you. Look up online to see how to use the function. (Note: the order of the argument is important!)\n",
    "- Repeat this process with model_deep.\n",
    "- Plot both scatter plot in the same figure, and include proper labels and legend.\n",
    "- Make sure the calculated R2 values are visible in the plot as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T2.4b IMPORTANT: Please do not edit/remove this comment.\n",
    "\n",
    "??? = model_shallow.???\n",
    "r2_shallow = r2_score(???)\n",
    "plt.scatter(???)\n",
    "\n",
    "??? = model_deep.???\n",
    "r2_deep = r2_score(???)\n",
    "plt.scatter(???)\n",
    "\n",
    "???"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
